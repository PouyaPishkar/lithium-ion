# -*- coding: utf-8 -*-
"""lithium-ion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17NeIAe2b_SJhTN0KKT0VQp7DsKqOrZyw
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR

# Load the dataset
file_path = '/content/metadata.csv'  # Replace with the actual path to your file
data = pd.read_csv(file_path)

# Preprocessing: Filter data with non-null Capacity and convert it to numeric
data_filtered = data.dropna(subset=['Capacity'])
data_filtered['Capacity'] = pd.to_numeric(data_filtered['Capacity'], errors='coerce')

# Drop any remaining NaN values after converting to numeric
data_filtered = data_filtered.dropna(subset=['Capacity'])

# Encode categorical columns
label_encoders = {}
for column in ['type', 'battery_id', 'filename']:
    le = LabelEncoder()
    data_filtered[column] = le.fit_transform(data_filtered[column])
    label_encoders[column] = le

# Define features and target
X = data_filtered[['type', 'ambient_temperature', 'battery_id', 'test_id']]
y = data_filtered['Capacity']

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    "Random Forest": RandomForestRegressor(random_state=42, n_estimators=100),
    "Linear Regression": LinearRegression(),
    "K-Nearest Neighbors (KNN)": KNeighborsRegressor(n_neighbors=5),
    "Support Vector Regression (SVR)": SVR(kernel='rbf')
}

# Dictionary to store results
results = {}

# Train each model, make predictions, and calculate metrics
for model_name, model in models.items():
    # Train model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Calculate metrics
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Store results
    results[model_name] = {'MSE': mse, 'R2': r2}

    # Print metrics
    print(f"{model_name}:")
    print(f"  Mean Squared Error: {mse:.4f}")
    print(f"  R² Score: {r2:.4f}")
    print('-' * 40)

# Plot comparison of models
model_names = list(results.keys())
mse_values = [results[model]['MSE'] for model in model_names]
r2_values = [results[model]['R2'] for model in model_names]

# Plot MSE
plt.figure(figsize=(10, 6))
sns.barplot(x=model_names, y=mse_values, palette='viridis')
plt.title('Comparison of Models (MSE)')
plt.xlabel('Model')
plt.ylabel('Mean Squared Error')
plt.grid(True)
plt.show()

# Plot R² Score
plt.figure(figsize=(10, 6))
sns.barplot(x=model_names, y=r2_values, palette='viridis')
plt.title('Comparison of Models (R² Score)')
plt.xlabel('Model')
plt.ylabel('R² Score')
plt.grid(True)
plt.show()





import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Load the dataset
file_path = '/content/metadata.csv'  # Replace with your file path
data = pd.read_csv(file_path)

# Preprocessing: Ensure Capacity is numeric and drop NaN values
data['Capacity'] = pd.to_numeric(data['Capacity'], errors='coerce')  # Convert to numeric
data_filtered = data.dropna(subset=['Capacity'])  # Drop rows with NaN in 'Capacity'

# Encode categorical columns
label_encoders = {}
for column in ['type', 'battery_id', 'filename']:
    le = LabelEncoder()
    data_filtered[column] = le.fit_transform(data_filtered[column])
    label_encoders[column] = le

# Define features and target
X = data_filtered[['type', 'ambient_temperature', 'battery_id', 'test_id']]
y = data_filtered['Capacity']

# Ensure no NaN values in the target variable
if y.isnull().sum() > 0:
    print(f"Number of NaN values in y: {y.isnull().sum()}")
    raise ValueError("Target variable 'y' contains NaN values after preprocessing.")

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Random Forest Regressor
model = RandomForestRegressor(random_state=42, n_estimators=100)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print metrics
print(f"Mean Squared Error: {mse:.4f}")
print(f"R2 Score: {r2:.4f}")

# Plot 1: Feature Importance
feature_importance = model.feature_importances_
plt.figure(figsize=(8, 6))
sns.barplot(x=feature_importance, y=X.columns)
plt.title('Feature Importance')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.show()

# Plot 2: Actual vs Predicted
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.7, color='b')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.title('Actual vs Predicted')
plt.xlabel('Actual Capacity')
plt.ylabel('Predicted Capacity')
plt.grid()
plt.show()

# Plot 3: Residuals
residuals = y_test - y_pred
plt.figure(figsize=(8, 6))
sns.histplot(residuals, kde=True, color='purple', bins=30)
plt.title('Distribution of Residuals')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.show()

# Plot 4: Error vs Predicted
plt.figure(figsize=(8, 6))
plt.scatter(y_pred, residuals, alpha=0.7, color='green')
plt.axhline(0, color='red', linestyle='--')
plt.title('Residuals vs Predicted Capacity')
plt.xlabel('Predicted Capacity')
plt.ylabel('Residuals')
plt.grid()
plt.show()


import pandas as pd
import ast
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
import matplotlib.pyplot as plt
import seaborn as sns

# Load your dataset
file_path = "/content/battery_cleaned2.csv"  # Change this to your file path
data = pd.read_csv(file_path)

# Check for missing values
print("Missing values:\n", data.isnull().sum())

# Drop rows with missing values in Extracted_name or Value
data.dropna(subset=['Extracted_name', 'Value'], inplace=True)

# Convert the string representations of lists of dictionaries into actual lists of dictionaries
def parse_formula(formula_str):
    try:
        return ast.literal_eval(formula_str)
    except (ValueError, SyntaxError):
        return []  # Return empty list if parsing fails

# Apply the parsing function to the Extracted_name column
data['Parsed_Extracted_name'] = data['Extracted_name'].apply(parse_formula)

# Flatten the list of dictionaries and combine the element counts
def flatten_formula(parsed_formula):
    flattened = {}
    for formula in parsed_formula:
        for element, count in formula.items():
            try:
                count = float(count)
            except ValueError:
                count = 0
            flattened[element] = flattened.get(element, 0) + count
    return flattened

# Apply the flattening function to create a new DataFrame with numerical features
flattened_data = data['Parsed_Extracted_name'].apply(flatten_formula)

# Convert the flattened dictionaries into a DataFrame, fill NaNs with 0
X = pd.DataFrame(flattened_data.tolist()).fillna(0)

# Target variable
y = data['Value']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[('num', StandardScaler(), X.columns)]  # Scale numerical columns
)

# List of models to compare
models = {
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'SVR': SVR(),
    'Linear Regression': LinearRegression(),
    'K-Nearest Neighbors': KNeighborsRegressor()
}

# Dictionary to store model performance metrics
model_metrics = {}

# Evaluate each model
for model_name, model in models.items():
    # Create a model pipeline with preprocessing + regressor
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('regressor', model)
    ])

    # Train the model
    pipeline.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = pipeline.predict(X_test)

    # Evaluate the model
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Store the metrics
    model_metrics[model_name] = {'MAE': mae, 'MSE': mse, 'R2': r2}

# Print model performance metrics
for model_name, metrics in model_metrics.items():
    print(f"\n{model_name} Model Performance:")
    print(f"Mean Absolute Error: {metrics['MAE']:.2f}")
    print(f"Mean Squared Error: {metrics['MSE']:.2f}")
    print(f"R² Score: {metrics['R2']:.2f}")

# Plot the model comparison (Bar plots for MAE, MSE, and R²)
metrics_df = pd.DataFrame(model_metrics).T

# Plot for MAE
plt.figure(figsize=(10, 6))
metrics_df['MAE'].plot(kind='bar', color='lightcoral')
plt.title('Mean Absolute Error Comparison')
plt.ylabel('Mean Absolute Error')
plt.show()

# Plot for MSE
plt.figure(figsize=(10, 6))
metrics_df['MSE'].plot(kind='bar', color='lightblue')
plt.title('Mean Squared Error Comparison')
plt.ylabel('Mean Squared Error')
plt.show()

# Plot for R² Score
plt.figure(figsize=(10, 6))
metrics_df['R2'].plot(kind='bar', color='lightgreen')
plt.title('R² Score Comparison')
plt.ylabel('R² Score')
plt.show()

# Optional: Cross-validation to evaluate model performance (Boxplot for MSE)
cv_scores_dict = {}
for model_name, model in models.items():
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('regressor', model)
    ])

    # Cross-validation to evaluate the model performance
    cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='neg_mean_squared_error')
    cv_scores_dict[model_name] = -cv_scores  # negating to make it positive

# Plot the cross-validation MSE distribution (Boxplot)
cv_scores_df = pd.DataFrame(cv_scores_dict)

plt.figure(figsize=(10, 6))
sns.boxplot(data=cv_scores_df)
plt.title('Cross-Validation MSE Comparison')
plt.ylabel('MSE')
plt.show()

import pandas as pd
import ast
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
import matplotlib.pyplot as plt
import seaborn as sns

# Load your dataset
file_path = "/content/battery_cleaned2.csv"  # Change this to your file path
data = pd.read_csv(file_path)

# Check for missing values
print("Missing values:\n", data.isnull().sum())

# Drop rows with missing values in Extracted_name or Value
data.dropna(subset=['Extracted_name', 'Value'], inplace=True)

# Convert the string representations of lists of dictionaries into actual lists of dictionaries
def parse_formula(formula_str):
    try:
        return ast.literal_eval(formula_str)
    except (ValueError, SyntaxError):
        return []  # Return empty list if parsing fails

# Apply the parsing function to the Extracted_name column
data['Parsed_Extracted_name'] = data['Extracted_name'].apply(parse_formula)

# Flatten the list of dictionaries and combine the element counts
def flatten_formula(parsed_formula):
    flattened = {}
    for formula in parsed_formula:
        for element, count in formula.items():
            try:
                count = float(count)
            except ValueError:
                count = 0
            flattened[element] = flattened.get(element, 0) + count
    return flattened

# Apply the flattening function to create a new DataFrame with numerical features
flattened_data = data['Parsed_Extracted_name'].apply(flatten_formula)

# Convert the flattened dictionaries into a DataFrame, fill NaNs with 0
X = pd.DataFrame(flattened_data.tolist()).fillna(0)

# Target variable
y = data['Value']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[('num', StandardScaler(), X.columns)]  # Scale numerical columns
)

# Create a model pipeline with preprocessing + RandomForestRegressor
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))
])

# Train the model
pipeline.fit(X_train, y_train)

# Make predictions on the test set
y_pred = pipeline.predict(X_test)

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Random Forest Model Performance:")
print(f"Mean Absolute Error: {mae:.2f}")
print(f"Mean Squared Error: {mse:.2f}")
print(f"R² Score: {r2:.2f}")

# Optional: Cross-validation to evaluate model performance
cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='neg_mean_squared_error')
print(f"Cross-Validation MSE Scores: {-cv_scores}")
print(f"Mean Cross-Validation MSE: {-cv_scores.mean():.2f}")

# --- Plotting Section ---
# 1. Actual vs Predicted values (Scatter Plot)
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.7)
plt.title('Actual vs Predicted Values (Random Forest)')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=2)  # Diagonal line
plt.show()
plt.figtext(0.5, 0.01, f"R²: {r2:.2f} | MAE: {mae:.2f} | MSE: {mse:.2f}", ha='center', fontsize=10)

# 2. Residual Plot (difference between actual and predicted values)
residuals = y_test - y_pred
plt.figure(figsize=(8, 6))
sns.residplot(x=y_pred, y=residuals, lowess=True, line_kws={'color': 'red'})
plt.title('Residuals vs Predicted (Random Forest)')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.show()
plt.figtext(0.5, 0.01, f"R²: {r2:.2f} | MAE: {mae:.2f} | MSE: {mse:.2f}", ha='center', fontsize=10)

# 3. Feature Importances from the RandomForest model
importances = pipeline.named_steps['regressor'].feature_importances_
features = X.columns

# Create a DataFrame for feature importances
feature_importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': importances
})

# Sort the DataFrame by importance
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
plt.title('Feature Importance (Random Forest)')
plt.show()
plt.figtext(0.5, 0.01, f"R²: {r2:.2f} | MAE: {mae:.2f} | MSE: {mse:.2f}", ha='center', fontsize=10)

# 4. Cross-validation MSE distribution (Boxplot)
plt.figure(figsize=(8, 6))
sns.boxplot(data=-cv_scores)
plt.title('Cross-Validation MSE Scores (Random Forest)')
plt.xlabel('MSE')
plt.show()
plt.figtext(0.5, 0.01, f"R²: {r2:.2f} | MAE: {mae:.2f} | MSE: {mse:.2f}", ha='center', fontsize=10)

# 5. Distribution of Target Variable (Value)
plt.figure(figsize=(8, 6))
sns.histplot(y, kde=True, bins=20)
plt.title('Distribution of Target Variable (Value)')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.show()
plt.figtext(0.5, 0.01, f"R²: {r2:.2f} | MAE: {mae:.2f} | MSE: {mse:.2f}", ha='center', fontsize=10)

# 6. Distribution of Predictions
plt.figure(figsize=(8, 6))
sns.histplot(y_pred, kde=True, bins=20)
plt.title('Distribution of Predicted Values (Random Forest)')
plt.xlabel('Predicted Value')
plt.ylabel('Frequency')
plt.show()
plt.figtext(0.5, 0.01, f"R²: {r2:.2f} | MAE: {mae:.2f} | MSE: {mse:.2f}", ha='center', fontsize=10)

# 7. Comparison of Actual vs Predicted (Bar Plot)
comparison_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
comparison_df = comparison_df.head(20)  # Show only top 20 comparisons for visualization
comparison_df.plot(kind='bar', figsize=(10, 6))
plt.title('Comparison of Actual vs Predicted (Top 20 Predictions)')
plt.ylabel('Value')
plt.show()
plt.figtext(0.5, 0.01, f"R²: {r2:.2f} | MAE: {mae:.2f} | MSE: {mse:.2f}", ha='center', fontsize=10)

# 8. Residuals Histogram
plt.figure(figsize=(8, 6))
sns.histplot(residuals, kde=True, bins=20, color='red')
plt.title('Residuals Histogram')
plt.xlabel('Residuals (Actual - Predicted)')
plt.ylabel('Frequency')
plt.show()
plt.figtext(0.5, 0.01, f"R²: {r2:.2f} | MAE: {mae:.2f} | MSE: {mse:.2f}", ha='center', fontsize=10)

# 9. Actual vs Predicted (Hexbin Plot for dense visualization)
plt.figure(figsize=(8, 6))
plt.hexbin(y_test, y_pred, gridsize=30, cmap='Blues')
plt.title('Actual vs Predicted (Hexbin Plot)')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.colorbar(label='Count')
plt.show()
plt.figtext(0.5, 0.01, f"R²: {r2:.2f} | MAE: {mae:.2f} | MSE: {mse:.2f}", ha='center', fontsize=10)

# 10. Model Performance Summary (MAE, MSE, R² score)
metrics = pd.DataFrame({
    'Metric': ['Mean Absolute Error', 'Mean Squared Error', 'R² Score'],
    'Value': [mae, mse, r2]
})

plt.figure(figsize=(8, 6))
sns.barplot(x='Metric', y='Value', data=metrics, palette='viridis')
plt.title('Model Performance Metrics')
plt.show()
plt.figtext(0.5, 0.01, f"R²: {r2:.2f} | MAE: {mae:.2f} | MSE: {mse:.2f}", ha='center', fontsize=10)




import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Load dataset
file_path = "/content/New Microsoft Excel Worksheet.csv"  # Your file path
data = pd.read_csv(file_path)

# Drop rows with missing values in 'Formula' or 'Volume'
data.dropna(subset=['Formula', 'Volume'], inplace=True)

# Function to parse the formula and extract element counts
def parse_formula(formula_str):
    elements = {}
    i = 0
    while i < len(formula_str):
        element = formula_str[i]
        if i + 1 < len(formula_str) and formula_str[i + 1].islower():
            element = formula_str[i:i + 2]
            i += 2
        else:
            i += 1
        count = ""
        while i < len(formula_str) and formula_str[i].isdigit():
            count += formula_str[i]
            i += 1
        if count == "":
            count = 1
        else:
            count = int(count)
        elements[element] = elements.get(element, 0) + count
    return elements

# Apply the parsing function to the 'Formula' column
data['Parsed_Formula'] = data['Formula'].apply(parse_formula)

# Flatten the parsed formulas into a feature matrix
def flatten_formula(parsed_formula):
    flattened = {}
    for element, count in parsed_formula.items():
        flattened[element] = count
    return flattened

flattened_data = data['Parsed_Formula'].apply(flatten_formula)

# Convert the flattened data into a DataFrame, filling missing values with 0
X_formula = pd.DataFrame(flattened_data.tolist()).fillna(0)

# Extract numerical columns
X_numerical = data[['Formation Energy (eV)', 'E Above Hull (eV)', 'Band Gap (eV)', 'Nsites']]

# Handle categorical 'Crystal System' by one-hot encoding
X_categorical = data[['Crystal System']]
X_categorical_encoded = pd.get_dummies(X_categorical, drop_first=True)  # One-hot encoding

# Combine all features (excluding 'Has Bandstructure' and 'Spacegroup')
X = pd.concat([X_formula, X_numerical, X_categorical_encoded], axis=1)

# Target variable: Volume
y = data['Volume']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a model pipeline
pipeline = Pipeline(steps=[
    ('scaler', StandardScaler()),  # Feature scaling
    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))  # Using RandomForestRegressor with parallelism
])

# GridSearchCV for hyperparameter tuning
param_grid = {
    'regressor__n_estimators': [100, 150],
    'regressor__max_depth': [None, 10, 20],
    'regressor__min_samples_split': [2, 5],
    'regressor__min_samples_leaf': [1, 2],
    'regressor__bootstrap': [True]
}

# Fit the grid search
grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)

# Get the best model
best_model = grid_search.best_estimator_

# Make predictions on the test set using the best model
y_pred = best_model.predict(X_test)

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Best hyperparameters: {grid_search.best_params_}")
print(f"Mean Absolute Error: {mae:.2f}")
print(f"Mean Squared Error: {mse:.2f}")
print(f"R² Score: {r2:.2f}")

# Function to predict Volume based on user input for a new material
def predict_volume(formula, formation_energy, e_above_hull, band_gap, nsites, crystal_system):
    parsed_formula = parse_formula(formula)
    flattened_formula = flatten_formula(parsed_formula)
    input_data = pd.DataFrame([flattened_formula]).fillna(0)
    input_data['Formation Energy (eV)'] = formation_energy
    input_data['E Above Hull (eV)'] = e_above_hull
    input_data['Band Gap (eV)'] = band_gap
    input_data['Nsites'] = nsites
    crystal_system_encoded = pd.get_dummies(pd.DataFrame({'Crystal System': [crystal_system]}), drop_first=True)
    final_input = pd.concat([input_data, crystal_system_encoded], axis=1).fillna(0)
    final_input = final_input.reindex(columns=X.columns, fill_value=0)
    volume_prediction = best_model.predict(final_input)
    return volume_prediction[0]

# Plotting section - 12 plots

# 1. Distribution of numerical features
numerical_columns = X_numerical.columns
plt.figure(figsize=(12, 8))
for i, col in enumerate(numerical_columns, 1):
    plt.subplot(2, 2, i)
    sns.histplot(data[col], kde=True)
    plt.title(f'{col} Distribution')
plt.tight_layout()
plt.show()

# 2. Correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(X.corr(), annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Feature Correlation Heatmap')
plt.show()

# 3. Feature Importances
feature_importances = best_model.named_steps['regressor'].feature_importances_
plt.figure(figsize=(10, 8))
sns.barplot(x=X.columns, y=feature_importances)
plt.xticks(rotation=90)
plt.title('Feature Importances')
plt.show()

# 4. Training vs Test Set Distribution
plt.figure(figsize=(12, 6))
sns.histplot(y_train, color='blue', label='Train', kde=True, stat="density", linewidth=0)
sns.histplot(y_test, color='red', label='Test', kde=True, stat="density", linewidth=0)
plt.title('Training vs Test Set Distribution')
plt.legend()
plt.show()

# 5. Residuals (Prediction Errors)
plt.figure(figsize=(12, 6))
sns.histplot(y_test - y_pred, kde=True, color='orange')
plt.title('Residuals (Prediction Errors)')
plt.show()

# 6. True vs Predicted
plt.figure(figsize=(12, 6))
sns.scatterplot(x=y_test, y=y_pred)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')  # Ideal line
plt.title('True vs Predicted Values')
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
plt.show()

# 8. Cross-validation MSE Scores
cv_scores = cross_val_score(best_model, X, y, cv=3, scoring='neg_mean_squared_error')
plt.figure(figsize=(8, 6))
sns.boxplot(data=-cv_scores)
plt.title('Cross-validation MSE Scores')
plt.ylabel('Mean Squared Error')
plt.show()

# 9. Boxplots for Numerical Features
plt.figure(figsize=(12, 8))
for i, col in enumerate(numerical_columns, 1):
    plt.subplot(2, 2, i)
    sns.boxplot(x=data[col])
    plt.title(f'{col} Boxplot')
plt.tight_layout()
plt.show()

# 10. Pairplot of Numerical Features
sns.pairplot(X_numerical)
plt.suptitle('Pairplot of Numerical Features', y=1.02)
plt.show()

# 12. Distribution of Predicted vs Actual
plt.figure(figsize=(12, 6))
sns.histplot(y_test, kde=True, color='blue', label='True Values')
sns.histplot(y_pred, kde=True, color='red', label='Predicted Values')
plt.title('Distribution of Predicted vs Actual Volume')
plt.legend()
plt.show()
import pandas as pd
import ast
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load your dataset
file_path = "/content/New Microsoft Excel Worksheet.csv"  # Your file path
data = pd.read_csv(file_path)

# Drop rows with missing values in 'Formula' or 'Volume'
data.dropna(subset=['Formula', 'Volume'], inplace=True)

# Function to parse the formula and extract element counts
def parse_formula(formula_str):
    elements = {}
    i = 0
    while i < len(formula_str):
        element = formula_str[i]
        if i + 1 < len(formula_str) and formula_str[i + 1].islower():
            element = formula_str[i:i + 2]
            i += 2
        else:
            i += 1
        count = ""
        while i < len(formula_str) and formula_str[i].isdigit():
            count += formula_str[i]
            i += 1
        if count == "":
            count = 1
        else:
            count = int(count)
        elements[element] = elements.get(element, 0) + count
    return elements

# Apply the parsing function to the 'Formula' column
data['Parsed_Formula'] = data['Formula'].apply(parse_formula)

# Flatten the parsed formulas into a feature matrix
def flatten_formula(parsed_formula):
    flattened = {}
    for element, count in parsed_formula.items():
        flattened[element] = count
    return flattened

flattened_data = data['Parsed_Formula'].apply(flatten_formula)

# Convert the flattened data into a DataFrame, filling missing values with 0
X_formula = pd.DataFrame(flattened_data.tolist()).fillna(0)

# Extract numerical columns
X_numerical = data[['Formation Energy (eV)', 'E Above Hull (eV)', 'Band Gap (eV)', 'Nsites', 'Volume']]

# Handle categorical 'Crystal System' by one-hot encoding
X_categorical = data[['Crystal System']]
X_categorical_encoded = pd.get_dummies(X_categorical, drop_first=True)  # One-hot encoding

# Combine all features (no 'Has Bandstructure' or 'Spacegroup')
X = pd.concat([X_formula, X_numerical, X_categorical_encoded], axis=1)

# Target variable: Volume
y = data['Volume']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a dictionary to store models
models = {
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(random_state=42),
    'SVR': SVR(),
    'Linear Regression': LinearRegression(),
    'K-Nearest Neighbors': KNeighborsRegressor()
}

# Dictionary to store model performance metrics
model_performance = {
    'Model': [],
    'MAE': [],
    'MSE': [],
    'R² Score': []
}

# Loop through models, train, and evaluate
for model_name, model in models.items():
    # Create pipeline with standard scaler and regressor
    pipeline = Pipeline(steps=[('scaler', StandardScaler()), ('regressor', model)])

    # Fit the model
    pipeline.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = pipeline.predict(X_test)

    # Calculate evaluation metrics
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Store the metrics
    model_performance['Model'].append(model_name)
    model_performance['MAE'].append(mae)
    model_performance['MSE'].append(mse)
    model_performance['R² Score'].append(r2)

# Convert model performance dictionary to a DataFrame
performance_df = pd.DataFrame(model_performance)

# Print performance metrics
print(performance_df)

# Plot the performance metrics for comparison
plt.figure(figsize=(14, 6))

# Plot MAE
plt.subplot(1, 3, 1)
sns.barplot(x='Model', y='MAE', data=performance_df)
plt.title('Mean Absolute Error (MAE)')

# Plot MSE
plt.subplot(1, 3, 2)
sns.barplot(x='Model', y='MSE', data=performance_df)
plt.title('Mean Squared Error (MSE)')

# Plot R² Score
plt.subplot(1, 3, 3)
sns.barplot(x='Model', y='R² Score', data=performance_df)
plt.title('R² Score')

# Display plots
plt.tight_layout()
plt.show()




import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Load dataset
file_path = "/content/New Microsoft Excel Worksheet.csv"  # Your file path
data = pd.read_csv(file_path)

# Drop rows with missing values in 'Formula' or 'Volume'
data.dropna(subset=['Formula', 'Volume'], inplace=True)

# Function to parse the formula and extract element counts
def parse_formula(formula_str):
    elements = {}
    i = 0
    while i < len(formula_str):
        element = formula_str[i]
        if i + 1 < len(formula_str) and formula_str[i + 1].islower():
            element = formula_str[i:i + 2]
            i += 2
        else:
            i += 1
        count = ""
        while i < len(formula_str) and formula_str[i].isdigit():
            count += formula_str[i]
            i += 1
        if count == "":
            count = 1
        else:
            count = int(count)
        elements[element] = elements.get(element, 0) + count
    return elements

# Apply the parsing function to the 'Formula' column
data['Parsed_Formula'] = data['Formula'].apply(parse_formula)

# Flatten the parsed formulas into a feature matrix
def flatten_formula(parsed_formula):
    flattened = {}
    for element, count in parsed_formula.items():
        flattened[element] = count
    return flattened

flattened_data = data['Parsed_Formula'].apply(flatten_formula)

# Convert the flattened data into a DataFrame, filling missing values with 0
X_formula = pd.DataFrame(flattened_data.tolist()).fillna(0)

# Extract numerical columns
X_numerical = data[['Formation Energy (eV)', 'E Above Hull (eV)', 'Band Gap (eV)', 'Nsites']]

# Handle categorical 'Crystal System' by one-hot encoding
X_categorical = data[['Crystal System']]
X_categorical_encoded = pd.get_dummies(X_categorical, drop_first=True)  # One-hot encoding

# Combine all features (excluding 'Has Bandstructure' and 'Spacegroup')
X = pd.concat([X_formula, X_numerical, X_categorical_encoded], axis=1)

# Target variable: Volume
y = data['Volume']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a model pipeline
pipeline = Pipeline(steps=[
    ('scaler', StandardScaler()),  # Feature scaling
    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))  # Using RandomForestRegressor with parallelism
])

# GridSearchCV for hyperparameter tuning
param_grid = {
    'regressor__n_estimators': [100, 150],
    'regressor__max_depth': [None, 10, 20],
    'regressor__min_samples_split': [2, 5],
    'regressor__min_samples_leaf': [1, 2],
    'regressor__bootstrap': [True]
}

# Fit the grid search
grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)

# Get the best model
best_model = grid_search.best_estimator_

# Make predictions on the test set using the best model
y_pred = best_model.predict(X_test)

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Best hyperparameters: {grid_search.best_params_}")
print(f"Mean Absolute Error: {mae:.2f}")
print(f"Mean Squared Error: {mse:.2f}")
print(f"R² Score: {r2:.2f}")

# Function to predict Volume based on user input for a new material
def predict_volume(formula, formation_energy, e_above_hull, band_gap, nsites, crystal_system):
    parsed_formula = parse_formula(formula)
    flattened_formula = flatten_formula(parsed_formula)
    input_data = pd.DataFrame([flattened_formula]).fillna(0)
    input_data['Formation Energy (eV)'] = formation_energy
    input_data['E Above Hull (eV)'] = e_above_hull
    input_data['Band Gap (eV)'] = band_gap
    input_data['Nsites'] = nsites
    crystal_system_encoded = pd.get_dummies(pd.DataFrame({'Crystal System': [crystal_system]}), drop_first=True)
    final_input = pd.concat([input_data, crystal_system_encoded], axis=1).fillna(0)
    final_input = final_input.reindex(columns=X.columns, fill_value=0)
    volume_prediction = best_model.predict(final_input)
    return volume_prediction[0]

# Plotting section - 12 plots

# 1. Distribution of numerical features
numerical_columns = X_numerical.columns
plt.figure(figsize=(12, 8))
for i, col in enumerate(numerical_columns, 1):
    plt.subplot(2, 2, i)
    sns.histplot(data[col], kde=True)
    plt.title(f'{col} Distribution')
plt.tight_layout()
plt.show()

# 2. Correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(X.corr(), annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Feature Correlation Heatmap')
plt.show()

# 3. Feature Importances
feature_importances = best_model.named_steps['regressor'].feature_importances_
plt.figure(figsize=(10, 8))
sns.barplot(x=X.columns, y=feature_importances)
plt.xticks(rotation=90)
plt.title('Feature Importances')
plt.show()

# 4. Training vs Test Set Distribution
plt.figure(figsize=(12, 6))
sns.histplot(y_train, color='blue', label='Train', kde=True, stat="density", linewidth=0)
sns.histplot(y_test, color='red', label='Test', kde=True, stat="density", linewidth=0)
plt.title('Training vs Test Set Distribution')
plt.legend()
plt.show()

# 5. Residuals (Prediction Errors)
plt.figure(figsize=(12, 6))
sns.histplot(y_test - y_pred, kde=True, color='orange')
plt.title('Residuals (Prediction Errors)')
plt.show()

# 6. True vs Predicted
plt.figure(figsize=(12, 6))
sns.scatterplot(x=y_test, y=y_pred)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')  # Ideal line
plt.title('True vs Predicted Values')
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
plt.show()

# 8. Cross-validation MSE Scores
cv_scores = cross_val_score(best_model, X, y, cv=3, scoring='neg_mean_squared_error')
plt.figure(figsize=(8, 6))
sns.boxplot(data=-cv_scores)
plt.title('Cross-validation MSE Scores')
plt.ylabel('Mean Squared Error')
plt.show()

# 9. Boxplots for Numerical Features
plt.figure(figsize=(12, 8))
for i, col in enumerate(numerical_columns, 1):
    plt.subplot(2, 2, i)
    sns.boxplot(x=data[col])
    plt.title(f'{col} Boxplot')
plt.tight_layout()
plt.show()

# 10. Pairplot of Numerical Features
sns.pairplot(X_numerical)
plt.suptitle('Pairplot of Numerical Features', y=1.02)
plt.show()

# 12. Distribution of Predicted vs Actual
plt.figure(figsize=(12, 6))
sns.histplot(y_test, kde=True, color='blue', label='True Values')
sns.histplot(y_pred, kde=True, color='red', label='Predicted Values')
plt.title('Distribution of Predicted vs Actual Volume')
plt.legend()
plt.show()
import pandas as pd
import ast
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load your dataset
file_path = "/content/New Microsoft Excel Worksheet.csv"  # Your file path
data = pd.read_csv(file_path)

# Drop rows with missing values in 'Formula' or 'Volume'
data.dropna(subset=['Formula', 'Volume'], inplace=True)

# Function to parse the formula and extract element counts
def parse_formula(formula_str):
    elements = {}
    i = 0
    while i < len(formula_str):
        element = formula_str[i]
        if i + 1 < len(formula_str) and formula_str[i + 1].islower():
            element = formula_str[i:i + 2]
            i += 2
        else:
            i += 1
        count = ""
        while i < len(formula_str) and formula_str[i].isdigit():
            count += formula_str[i]
            i += 1
        if count == "":
            count = 1
        else:
            count = int(count)
        elements[element] = elements.get(element, 0) + count
    return elements

# Apply the parsing function to the 'Formula' column
data['Parsed_Formula'] = data['Formula'].apply(parse_formula)

# Flatten the parsed formulas into a feature matrix
def flatten_formula(parsed_formula):
    flattened = {}
    for element, count in parsed_formula.items():
        flattened[element] = count
    return flattened

flattened_data = data['Parsed_Formula'].apply(flatten_formula)

# Convert the flattened data into a DataFrame, filling missing values with 0
X_formula = pd.DataFrame(flattened_data.tolist()).fillna(0)

# Extract numerical columns
X_numerical = data[['Formation Energy (eV)', 'E Above Hull (eV)', 'Band Gap (eV)', 'Nsites', 'Volume']]

# Handle categorical 'Crystal System' by one-hot encoding
X_categorical = data[['Crystal System']]
X_categorical_encoded = pd.get_dummies(X_categorical, drop_first=True)  # One-hot encoding

# Combine all features (no 'Has Bandstructure' or 'Spacegroup')
X = pd.concat([X_formula, X_numerical, X_categorical_encoded], axis=1)

# Target variable: Volume
y = data['Volume']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a dictionary to store models
models = {
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(random_state=42),
    'SVR': SVR(),
    'Linear Regression': LinearRegression(),
    'K-Nearest Neighbors': KNeighborsRegressor()
}

# Dictionary to store model performance metrics
model_performance = {
    'Model': [],
    'MAE': [],
    'MSE': [],
    'R² Score': []
}

# Loop through models, train, and evaluate
for model_name, model in models.items():
    # Create pipeline with standard scaler and regressor
    pipeline = Pipeline(steps=[('scaler', StandardScaler()), ('regressor', model)])

    # Fit the model
    pipeline.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = pipeline.predict(X_test)

    # Calculate evaluation metrics
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Store the metrics
    model_performance['Model'].append(model_name)
    model_performance['MAE'].append(mae)
    model_performance['MSE'].append(mse)
    model_performance['R² Score'].append(r2)

# Convert model performance dictionary to a DataFrame
performance_df = pd.DataFrame(model_performance)

# Print performance metrics
print(performance_df)

# Plot the performance metrics for comparison
plt.figure(figsize=(14, 6))

# Plot MAE
plt.subplot(1, 3, 1)
sns.barplot(x='Model', y='MAE', data=performance_df)
plt.title('Mean Absolute Error (MAE)')

# Plot MSE
plt.subplot(1, 3, 2)
sns.barplot(x='Model', y='MSE', data=performance_df)
plt.title('Mean Squared Error (MSE)')

# Plot R² Score
plt.subplot(1, 3, 3)
sns.barplot(x='Model', y='R² Score', data=performance_df)
plt.title('R² Score')

# Display plots
plt.tight_layout()
plt.show()
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.inspection import permutation_importance

# Load your dataset
file_path = "/content/New Microsoft Excel Worksheet.csv"  # Replace with your dataset path
data = pd.read_csv(file_path)

# Drop rows with missing values in 'Formula' or 'Band Gap (eV)'
data.dropna(subset=['Formula', 'Band Gap (eV)'], inplace=True)

# Function to parse the formula and extract element counts
def parse_formula(formula_str):
    elements = {}
    i = 0
    while i < len(formula_str):
        element = formula_str[i]
        if i + 1 < len(formula_str) and formula_str[i + 1].islower():
            element = formula_str[i:i + 2]
            i += 2
        else:
            i += 1
        count = ""
        while i < len(formula_str) and formula_str[i].isdigit():
            count += formula_str[i]
            i += 1
        if count == "":
            count = 1
        else:
            count = int(count)
        elements[element] = elements.get(element, 0) + count
    return elements

# Apply the parsing function to the 'Formula' column
data['Parsed_Formula'] = data['Formula'].apply(parse_formula)

# Flatten the parsed formulas into a feature matrix
def flatten_formula(parsed_formula):
    flattened = {}
    for element, count in parsed_formula.items():
        flattened[element] = count
    return flattened

flattened_data = data['Parsed_Formula'].apply(flatten_formula)

# Convert the flattened data into a DataFrame, filling missing values with 0
X_formula = pd.DataFrame(flattened_data.tolist()).fillna(0)

# Extract numerical columns
X_numerical = data[['Formation Energy (eV)', 'E Above Hull (eV)', 'Nsites', 'Volume']]

# Handle categorical 'Crystal System' by one-hot encoding
X_categorical = data[['Crystal System']]
X_categorical_encoded = pd.get_dummies(X_categorical, drop_first=True)  # One-hot encoding

# Combine all features
X = pd.concat([X_formula, X_numerical, X_categorical_encoded], axis=1)

# Target variable: Band Gap (eV)
y = data['Band Gap (eV)']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create the model (RandomForestRegressor in this case)
model = RandomForestRegressor(n_estimators=100, random_state=42)

# Create a pipeline with feature scaling
pipeline = Pipeline(steps=[('scaler', StandardScaler()), ('regressor', model)])

# Fit the model
pipeline.fit(X_train, y_train)

# Make predictions on the test set
y_pred = pipeline.predict(X_test)

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Absolute Error: {mae:.2f}")
print(f"Mean Squared Error: {mse:.2f}")
print(f"R² Score: {r2:.2f}")

# Plotting

# 1. Feature Importance Plot (Random Forest)
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]
plt.figure(figsize=(10, 6))
plt.title("Feature Importance (Random Forest)")
plt.bar(range(X.shape[1]), importances[indices], align="center")
plt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)
plt.tight_layout()
plt.show()

# 2. Actual vs Predicted Band Gap (Test Set)
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, color='blue', edgecolor='black', alpha=0.7)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.title("Actual vs Predicted Band Gap")
plt.xlabel("Actual Band Gap (eV)")
plt.ylabel("Predicted Band Gap (eV)")
plt.tight_layout()
plt.show()

# 3. Residuals Plot
residuals = y_test - y_pred
plt.figure(figsize=(8, 6))
plt.scatter(y_pred, residuals, color='green', edgecolor='black', alpha=0.7)
plt.hlines(0, min(y_pred), max(y_pred), color='red', linestyle='--')
plt.title("Residuals Plot")
plt.xlabel("Predicted Band Gap (eV)")
plt.ylabel("Residuals")
plt.tight_layout()
plt.show()

# 4. Training vs Testing Loss Plot
train_pred = pipeline.predict(X_train)
plt.figure(figsize=(8, 6))
plt.plot(np.arange(len(y_train)), y_train - train_pred, label="Training Loss", alpha=0.7)
plt.plot(np.arange(len(y_test)), y_test - y_pred, label="Testing Loss", alpha=0.7)
plt.title("Training vs Testing Loss")
plt.xlabel("Sample Index")
plt.ylabel("Loss (Error)")
plt.legend()
plt.tight_layout()
plt.show()

# 5. Correlation Matrix of Features
correlation_matrix = X.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Correlation Matrix of Features")
plt.tight_layout()
plt.show()

# 7. Distribution of Band Gap (Actual vs Predicted)
plt.figure(figsize=(8, 6))
plt.hist(y_test, bins=20, alpha=0.7, label="Actual Band Gap")
plt.hist(y_pred, bins=20, alpha=0.7, label="Predicted Band Gap")
plt.title("Distribution of Actual vs Predicted Band Gap")
plt.xlabel("Band Gap (eV)")
plt.ylabel("Frequency")
plt.legend()
plt.tight_layout()
plt.show()

# 8. Histogram of Errors (Residuals)
plt.figure(figsize=(8, 6))
plt.hist(residuals, bins=30, edgecolor='black', alpha=0.7)
plt.title("Histogram of Errors (Residuals)")
plt.xlabel("Residuals (Error)")
plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

# 9. Prediction vs Ground Truth
plt.figure(figsize=(8, 6))
plt.plot(y_test.values, y_pred, 'o', label="Predictions", color='blue')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r-', label="Ground Truth")
plt.xlabel("Ground Truth Band Gap")
plt.ylabel("Predicted Band Gap")
plt.title("Prediction vs Ground Truth")
plt.legend()
plt.tight_layout()
plt.show()

# 10. Actual vs Predicted Band Gap for a Single Sample
plt.figure(figsize=(8, 6))
plt.scatter(range(len(y_test)), y_test, label="Actual Band Gap", color='blue', alpha=0.7)
plt.scatter(range(len(y_pred)), y_pred, label="Predicted Band Gap", color='red', alpha=0.7)
plt.title("Actual vs Predicted Band Gap for a Single Sample")
plt.xlabel("Sample Index")
plt.ylabel("Band Gap (eV)")
plt.legend()
plt.tight_layout()
plt.show()

# 11. Learning Curve (Training and Validation Error vs Training Size)
from sklearn.model_selection import learning_curve
train_sizes, train_scores, test_scores = learning_curve(pipeline, X, y, train_sizes=np.linspace(0.1, 1.0, 5), cv=3)
plt.figure(figsize=(8, 6))
plt.plot(train_sizes, np.mean(train_scores, axis=1), label="Training Error", color='blue')
plt.plot(train_sizes, np.mean(test_scores, axis=1), label="Validation Error", color='red')
plt.title("Learning Curve")
plt.xlabel("Training Set Size")
plt.ylabel("Error")
plt.legend()
plt.tight_layout()
plt.show()

# 12. Scatter Plot of Feature Importance
perm_importance = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)
plt.figure(figsize=(10, 6))
plt.barh(X.columns, perm_importance.importances_mean)
plt.title("Feature Importance (Permutation Importance)")
plt.tight_layout()
plt.show()